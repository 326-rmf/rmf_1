##  git stash
-   当你**正在进行项目中某一部分的工作**，里面的东西处于一个比较杂乱的状态，而你想转到其他分支上进行一些工作。问
    题是，**你不想提交进行了一半的工作，否则以后你无法回到这个工作点**。解决这个问题的办法就是git stash命令。**储藏(stash)可以获取你工作目录的中间状态——也就是你修改过的被追踪的文件和暂存的变更——并将它保存到一个未完结变更的堆栈中，随时可以重新应用。**
-   **git stash就可以将你当前未提交到本地（和服务器）的代码推入到Git的栈中**，这时候你的工作区间和上一次提交的内
    容是完全一样的，所以你可以放心的修Bug，等到修完Bug，提交到服务器上后，再使用**git stash apply将以前一半的工作应用回来。**
-   git stash pop命令**恢复之前缓存的工作目录**
    -   这个指令**将缓存堆栈中的第一个stash删除**，并将对应修改应用到当前的工作目录下
    -   git stash apply命令，**将缓存堆栈中的stash多次应用到工作目录中，但并不删除stash拷贝**。
    -   git stash apply命令时**可以通过名字指定使用哪个stash，默认使用最近的stash**
-   git stash drop stash@{0}
    -   git stash clear命令，**删除所有缓存的stash。**
-   如果**你储藏了一些工作，暂时不去理会，然后继续在你储藏工作的分支上工作**，你在**重新应用**工作时可能会碰到一
    些问题。如果尝试应用的变更是针对一个你那之后修改过的文件，你会碰到一个**归并冲突**并且必须去化解它。如果你想用更方便的方法来重新检验你储藏的变更，你可以**运行 git stash branch，这会创建一个新的分支，检出你储藏工作时的所处的提交，重新应用你的工作，如果成功，将会丢弃储藏。**
##  v-if    v-for
-    v-if 和 v-for 同时用在同一个元素上，带来**性能方面的浪费**（每次渲染都会先循环再进行条件判断）
    -   如果避免出现这种情况，则在外层嵌套template（页面渲染不生成dom节点），在这一层进行v-if判断，然后在内部进
        行v-for循环
    -   <template v-show='isShow'>
        <p v-for=''>
        </template>
    -   如果**条件出现在循环内部**，可通过计算属性**computed提前过滤掉那些不需要显示的项**
    -   computed:{
            items:function(){
                return this.arr.filter(function(item){
                    return item.isShow
                })
            }
        }
    -   过滤条件数组
##  组件重复打包
-   假设A.js文件是一个**常用的库，现在有多个路由使用了A.js文件，这就造成了重复下载**
    解决方案：在webpack的config文件中，修改**CommonsChunkPlugin**的配置  
##  为obj添加新属性的时候，却无法触发事件属性的拦截
-   obj的foo属性被设成了响应式数据，**而bar是后面新增的属性，并没有通过Object.defineProperty设置成响应式数据**
-   解决方案
    -   Vue.set()
        -   Vue.set(target,name/index,value)
        -   this.$set(target,name/index,value)
        -   原理
            -   **再次使用defineReactive方法**      来触发数据的响应式
            -   defineReactive内部还是使用Object.defineProperty来实现属性的拦截

    -   Object.assign()
    -   直接使用Object.assign()**添加到对象的新属性不会触发更新**
    -   **应创建一个新的对象，合并原对象和混入对象的属性**
    -   this.obj = Object.assign({},this.obj,{name:'Tom',age:18})

    -   $forcecUpdated()
    -   $forceUpdate迫使Vue **实例重新渲染**

##  插件通常用来为 Vue 添加全局功能

-   通过**全局混入来添加一些组件选项**。如vue-router

-   **添加 Vue 实例方法**，通过把它们添加到 Vue.prototype 上实现

### 编写组件

-   常见的就是vue单文件的这种格式

-   还可以通过**template属性来编写一个组件**，如果组件内容多，我们可以在外部定义template组件内容，如果组件内容
    并不多，我们可直接写在template属性上

    -   <template id='testComponent'>
        <div>component</div>
        </template>

    -   Vue.component('componentA',{
        id:'testComponent',
        template:`<div>component</div>`
        })

-   组件注册

    -   全局注册
        -   Vue.component('componentA',{})

    -   局部注册
        -   const component1 = {}
            components:{
                component1
            }

### 编写插件       

-   vue插件的实现应该暴露一个 **install 方法**。这个方法的**第一个参数是 Vue 构造器**，第二个参数是一个可选的
    选项对象

    -   myPlugin.js
        -   myPlugin.install = function (Vue, options) {

            Vue.globalMethod = function () {

            }

            Vue.directive('my-directive', {
                bind(el,binding,vnode,oldVnode) {

                }
            })

            Vue.mixin({
                created: function () {

                }
            })
        }

-   插件注册
    
    -   Vue.use(插件名称, {})

    -   注册插件的时候  需要在 new Vue() 启动应用之前完成

-   props 是子组件的属性

##  机器学习

-   人工智能概述        --》        机器学习        --》深度学习

    -   机器学习是人工智能的一个实现途径

    -   深度学习是机器学习的一个方法发展而来

-   达特茅斯会议        人工智能的起点

    -   1956年      人工智能的元年

-   机器学习        垃圾邮件过滤

-   人工神经网络是机器学习的一个方法

-   图像识别

-   机器学习    深度学习的应用

    -   传统预测

        -   店铺销量预测    量化投资    广告推荐

    -   图像识别

        -   人脸识别    道路标记识别

    -   自然语言处理
        
        -   文本分类    情感分析    自动聊天    

-   机器学习算法    切入业务领域解决问题

##  机器学习

-   从数据中自动分析获得模型    并利用模型对未知数据进行预测

##  数据集的构成

-   结构:   特征值  +   目标值

    -   根据特征值获取目标值

    -   每一行数据称之为样本

    -   有一些数据集可以没有目标值

-   可用的数据集

    -   scikit-learn    数据量小    方便学习

        -   python语言的机器学习工具

        -   安装

            -   python3.6.X

            -   https://www.lfd.uci.edu/~gohlke/pythonlibs/

            -   NumPy       SciPy       pyparsing       Matplotlib      scikit-learn

            -   pip install "numpy-1.19.5+mkl-cp36-cp36m-win_amd64.whl"

            -   pip install "scipy-1.5.4-cp36-cp36m-win_amd64.whl"

            -   pip install pyparsing-2.4.7-py3-none-any.whl

            -   pip install matplotlib-2.2.5-cp36-cp36m-win_amd64.whl
        
        -   分类    聚类     回归
        
        -   特征工程    
        
        -   模型选择    调优

-   scikit-learn    数据集API介绍

    -   sklearn.datasets

        -   **加载流行数据集**

        -   datasets.load_*()

            -   获取小规模的数据集  数据包含在datasets里面

        -   datasets.fetch_*(data_home=None)

            -   获取大规模的数据集  从网络下载  函数的第一个参数是 data_home 表示数据集下载的目录

                默认是  ~/scikit_learn_data/
        
-   sklearn小数据集

    -   sklearn.datasets.load_iris()

-   sklearn大数据集

    -   sklearn.datasets.fetch_20newsgroups(data_home=None,subset="train")

    -   train代表训练集     test代表测试集      all代表都需要

    -   加载并且返回鸢尾花数据集

-   数据集并不是全部都用来训练一个模型          因为还需要对**数据集训练的数据模型进行评估**

-   数据集的划分        一般划分为两个部分

    -   训练数据        用于训练    构建模型        

    -   测试数据        在模型检测中使用    用于评估模型是否有效

    -   73分        82分

-   数据集划分API

    -   sklearn.model_selection.train_test_split(arrays, *options)

        -   x数据集的特征值

        -   y数据集的标签值

        -   test_size测试集的大小   一般是float

        -   random_state随机数种子  不同的种子会造成不同的随机采样的结果    相同的种子采样结果是相同的

        -   return  训练集的特征值  测试集的特征值  训练集的目标值  测试集的目标值

### 特征工程

-   数据和特征决定了机器学习的上限  模型和算法逼近这个上线

-   专业知识和技巧处理数据          使得特征能在机器学习算法上发挥更好的作用

-   会影响机器学习的效果

-   pandas  数据清洗    数据处理

-   sklearn 特征工程        为特征工程提供了强大的接口

-   特征工程的步骤

    -   特征抽取

        -   将任意数据( 文本或者图像 )转换为机器学习的数字特征

        -   特征值化是为了计算机更好去理解数据

        -   字典特征提取

        -   文本特征提取

        -   图像特征提取
    
    -   特征提取API

        -   sklearn.feature_extraction

        -   字典特征提取        one-hot编码

            -   sklearn.feature_extraction.DictVectorizer( sparse = True,...)

                -   vector      数学:向量           物理:矢量

            -   DictVectorizer.fit_transform(X)

                -   X:字典或者包含字典的迭代器返回值    返回sparse矩阵

            -   DictVectorizer.inverse_transform(X)

                -   X:array数组或者sparse矩阵   返回值转化之前的数据格式

            -   DictVectorizer.get_feature_names()  返回类别的名称

            -   one-hot 编码

                -   不同的类别表示区分      用1 2 3 4这些不同数据区分会产生优先级的问题 不利于公平

                -   n个类别具有n列      数据具有相应的类别置1       否则置0     公平

            -   应用场景

                -   数据集中类别特征多

        -   文本特征提取

            -   单词作为特征

            -   文本特征抽取API

                -   sklearn.feature_extraction.text.CountVectorizer(stop_words=[])
                
                -   CountVectorizer.fit_transform(X)

                    -   X:文本或者包含文本字符串的可迭代对象        返回值:返回sparse矩阵

                -   CountVectorizer.inverse_transform(X)

                    -   X:array数组或者sparse矩阵       返回值:转化之前的数据格式

                -   CountVectorizer.get_feature_names()     返回值:单词列表

                -   统计样本特征词出现的个数        CountVectorizer

                -   中文文本特征抽取        需要对中文文本进行分词处理      不然会出现一句话是一个特征词问题

                -   停用词列表      stop_words=[]

                    -   对最终的数据训练没有帮助的数据      是不需要提取的

        -   中文文本特征抽取    自动分词的效果

            -   jieba可以中文文本自动分词

            -   在某一个类别的文章中    出现次数多  但在其他文章中出现少

            -   那么问题的出现在于解决关键词与否        解决方法是TfidfVectorizer

        -   Tf-idf文本特征提取

            -   主要思想:某个词或者短语在一篇文章中出现频率高   并且在其他文章中很少出现

            -   那么认为这种词或者短语具有很好的区分类别能力    适合用来分类

            -   作用:

                -   评估一个字词对于一个文件集或一个语料库中的其中一份文件的重要程度

            -   词频    term frequency( tf )

                -   一个给定的词汇在该文件中出现的频率

            -   逆向文档频率    inverse document frequency( idf )

                -   总文件数目除以包含该词语的文件数目      再将结果得到的商取以10为底的对数

            -   Tf-idf = tf * idf

        -   sklearn.feature_extraction.text.TfidfVectorizer(stop_words=None,...)

            -   返回词的权重矩阵
                
                -   TfidfVectorizer.fit_transform(X)

                -   X:文本或者包含文本字符串的可迭代对象

                -   返回sparse矩阵
            
            -   TfidfVectorizer.inverse_transform(X)

                -   X:array数组或者sparse矩阵

                -   返回值是转换之前的数据格式

            -   TfidfVectorizer.get_feature_names()     返回单词列表

    -   特征预处理

        -   通过一些转化函数    将特征数据转换为更加适合算法模型的特征数据过程

        -   特征预处理API

            -   sklearn.preprocessing

        -   数值型数据的无量纲化

            -   无量纲化的原因

                -   特征的单位或者大小相差较大  或者某特征的方差相比其他特征要大出几个数量级

                -   容易影响支配目标结果        使得一些算法无法学习到其他特征

            -   归一化

                -   X' = x - min / ( max - min )

                    -   max     min         为一列中的最大值        最小值

                -   X'' = X' * ( mx - mi ) + mi     

                    -   mx      mi          指定区间值  默认值是[0,1]

            -   归一化API

                -   sklearn.preprocessing.MinMaxScaler(feature_range=(0,1),...)

                -   feature_range是可以自己设定的范围数据   最后特征数据归一化的范围在里面

                -   MinMaxScalar.fit_transform(X)

                    -   X:numpy array格式的数据 [n_samples, n_features]

                    -   返回值:转化后的形状相同的array

                -   归一化结果容易受到异常点的影响    鲁棒性( 稳定性 )较差  只适合传统精确小数据场景

                -   读取文件数据

                    -   pandas

            -   标准化

                -   X' = (x - mean) / y

                -   mean是平均值        y是标准差       标准差:数据的集中程度   离散程度

                -   出现异常点  由于具有一定的数据量    少量的异常点对于平均值的影响是不大的

                -   从而方差的改变较小
            
            -   标准化API

                -   sklearn.preprocessing.StandardScaler()

                    -   处理之后的数据  对于每一列的数据    所有数据都是聚集在均值0附近的

                    -   StandardScaler.fit_transform(X)

                        -   X:numpy array格式的数据

                        -   返回值          转换后的形状相同的array

                -   应用场景

                    -   样本数量足够        适合现代大数据环境

    -   特征降维

        -   维数: 嵌套的层数

        -   0维:标量        1维:向量        2维:矩阵        3维

        -   降维

            -   在某些限定的条件下      降低随机变量的个数  得到一组 "不相关" 的主变量的过程

                -   特征和特征之间是不相关的

                -   特征之间的相关性较强    那么算法学习预测会影响较大

        -   降维的两种方式

            -   特征选择

                -   数据中包含冗余  或 相关变量     ( 从原有的特征中选取主要特征 )

                -   方法

                    -   Filter  过滤式方法

                        -   方差选择法          ( 低方差过滤 )

                            -   sklearn.feature_selection

                            -   sklearn.feature_selection.VarianceThreshold(threshold = 0.0)

                            -   删除所有低方差特征

                                -   Variance.fit_transform(X)

                                -   X:numpy array格式的数据

                                -   返回值训练集差异低于threshold的特征将被删除 默认值是保留

                                -   所有非0方差特征 就是删除所有样本中具有形同值的特征

                            -   特征方差小      那么某个特征大多样本数据较为集中

                        -   相关系数法          

                            -   特征与特征之间的相关程度

                            -   皮尔逊相关系数

                                -   反映变量之间的相关关系的密切程度的统计指标

                                -   x和y的相关性

                                    -   xy乘积的加和乘以样本数量    -   x的加和乘以y的加和  --》分子

                                    -   x平方的加和乘以样本数量 - x的加和的平方 --》得出结果开方

                                    -   y平方的加和乘以样本数量 - y的加和的平方 --》得出结果开方

                                    -   两个开方的结果数据相乘作为分母

                                    -   ![img](../picture/皮尔逊相关系数.png)
                                
                                -   相关系数r是在[-1,1]

                                -   r > 0   两个变量是正相关的

                                -   r < 0   两个变量是负相关的

                                -   r = 0   两个变量无相关性 

                                -   |r| = 1 两个变量是完全相关的

                                -   |r|越接近1  两个变量相关性越紧密

                                -   |r| < 0.4   是低度相关

                                -   0.4 <= |r| < 0.7   是显著性相关

                                -   0.7 <= |r| < 1   是高度线性相关

                        -   相关系数API

                            -   from scipy.stats import pearsonr

                            -   x:(N,)array_like

                            -   y:(N,)array_like

                        -   特征和特征之间的相关性很高

                            -   选取其中一个

                            -   加权求和        特征值各占50%   得出新的数据

            -   主成分分析

                -   高纬数据转化为低维数据  过程可能会舍弃原有数据  创造新的变量

                -   作用

                    -   是数据维数压缩  尽可能降低原数据维数 ( 复杂度 ) 损失少量信息

                -   应用

                    -   回归分析或者聚类分析当中

                -   主成分分析API

                    -   sklearn.decomposition.PCA(n_components=None)

                    -   将数据分解为较低维数空间

                    -   n_components

                        -   小数表示保留百分之多少的意思

                        -   整数表示减少多少特征

                    -   PCA.fit_transform(X)

                        -   X:numpy array格式的数据

                    -   返回值:转换后指定维度的array

                    -   Embedded    嵌入式方法

                        -   决策树

                        -   正则化

                        -   深度学习

    -   UCI特点

        -   收录了360个数据集

        -   覆盖科学    生活    经济等领域

        -   数据量几十万

    -   kaggle

        -   大数据竞赛平台  

##  机器学习算法分类

-   目标值是

    -   类别        --》分类问题        ( 比如识别猫狗 )    是离散型数据    --》属于监督学习

    -   连续性数据      --》回归问题        ( 比如预测房屋价格 )        --》属性监督学习

    -   无目标值        --》无监督学习

##  监督学习    

-   分类

    -   k-近邻算法  贝叶斯分类  决策树  随机森林    逻辑回归

-   回归

    -   线性回归    岭回归

##  无监督学习

-   聚类k-means

##  机器学习开发流程

-   获取数据

-   数据处理    ( 数据不符合要求    缺失值 )

-   特征工程    ( 数据处理为学习算法更为接收的数据 )

-   机器学习算法进行训练        得到模型

-   模型评估

-   应用

##  机器学习重点

-   算法    --》数据和计算是基础

### 机器学习库和框架

-   pytorch     theano是tensorflow的前身      chainer

##  探究用户对物品类别的喜好细分降维

-   需要将用户表和商品表放在同一个表中      合并

-   找到user_id     aisle       交叉表和透视表

-   特征冗余过多    需要PCA降维

##  分类算法

-   目标值是类别的话        那么就是分类问题

-   sklearn的转换器和预估器

    -   转换器

        -   是特征工程的父类

            -   实例化一个转换器类      Transformer

            -   使用fit_transform       得到计算数据结果

    -   fit_transform()

        -   fit()       计算公式中的数据

        -   transform()         根据数据进行最终的转化      将数据代入公式

    -   估计器

        -   实例化一个估计器          estimator

        -   estimator.fit(x_train, y_train)         计算        调用完毕        模型生成

        -   estimator的模型评估

            -   直接比对真实值和预测值

                -   y_direct = estimator.predict(x_test)

                -   y_test == y_predict

            -   计算准确率

                -   estimator.score(x_test, y_test)
-   K-近邻算法  ( KNN )原理

    -   根据邻居来推断数据的类别

    -   如果一个在特征空间中的k个最相似( 即特征空间中最邻近 )的样本中的大多数属于某一个类别

    -   那么这个样本也属于这个类别

    -   k=1         容易受到异常点的影响

    -   确定谁是邻居

        -   欧式距离            a(a1, a2, a3)   b(b1, b2, b3)

        -   (a1-b1)^2+(a2-b2)^2+(a3-b3)^2       -->结果数据开方

        -   曼哈顿距离

        -   绝对值距离      |a1-b1|+|a2-b2|+|a3-b3|

        -   明可夫斯基距离

    -   k等于的数据是距离目标值数据最近的特征数据       

        -   继续从特征数据中选取占比大的特征数据类别来决定测试数据的类别

        -   k值数据过大的时候       当样本不均衡的时候  结果会受到影响

    -   KNN算法因为涉及到距离的计算     那么数据的异常点会对结果产生影响

        -   那么需要进行无量纲化的处理

    -   K-近邻算法API

        -   sklearn.neighbors.KNeighborsClassiflter(n_neighbors, algorithm='auto')

        -   n_neighbors查询默认使用的邻居数量

        -   algorithm是选择计算最近邻居的算法

            -   auto将尝试根据传递给fit方法的数据来决定适合的算法

-   鸢尾花种类的预测

    -   获取数据

    -   数据集划分

    -   特征工程

        -   标准化

    -   KNN预估器流程

    -   模型评估

-   KNN算法

    -   懒惰算法    对测试样本分类计算大    内存开销大

    -   必须指定K值     K值的选择不当精度是不能保证的

    -   使用场景是几千~几万的数据量

-   模型选择和调优

    -   什么是交叉验证

        -   将拿到的训练数据    分为训练集和验证集  比如数据分为四分    其中一份数据是验证集    

            -   那么会进行4次的测试 每一次更换不同的验证集  得到四组模型的结果

            -   将平均值作为最终结果    称为交叉验证

        -   被评估的模型更加的准确可信

    -   超参数搜索-网格搜索

        -   参数需要手动指定的参数      手动会比较麻烦  那么就预设几种超参数组合

        -   每一组超参数都采用交叉验证来进行评估    最后选出最优的参数组合建立模型

-   模型选择和调优API

    -   sklearn.model_selection.GridSearchCV(estimator, param_grid=None, cv=None)

        -   对预估器的指定参数值进行详尽搜索

        -   estimator 是估计器对象

        -   param_grid 是估计器参数

        -   cv 指定几折交叉验证

        -   fit() 输入训练数据

        -   score() 准确率

##  three.js

-   geometry

    -   创建网格    移动网格

    -   wireframe:true      网格线段

-   BoxBufferGeometry()

    -   gpu更好渲染 性能提升    体验提升

    -   存储 buffer 数据

-   dat.gui

-   纹理

    -   图像转化为纹理

    -   纹理加载器

        -   textureLoader

        -   路径很喜欢使用require('')

-   LoadingManager      加载管理器

-   图形的材料  material

    -   颜色的程序      shadow

    -   MeshBasicMaterial       材料实例化之后的        color 熟悉设置

        -   color 变为一个实例化的类     不可直接设置属性值  需要调用相应的API

        -   material.color = 'red'  --> 不可直接设置

        -   material.color = new THREE.color('#ff00ff') 或者    material.color.set('#ff00ff')

    -   材料的透明设置

        -   material.transparent = true

        -   material.opacity = 0.5

    -   材料双方向的出现

        -   material.side = THREE.DoubleSide

    -   平面图的突起        简单图形的坐标点是不足的

        -   那么需要增加坐标点  增加细节



